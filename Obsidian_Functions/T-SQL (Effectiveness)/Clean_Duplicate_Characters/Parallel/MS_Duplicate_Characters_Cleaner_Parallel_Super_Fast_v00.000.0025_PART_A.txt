--The following few blocks of code create a new stored procedure which can be used in order to remove multiple consecutive duplicate characters from strings.
Declare @Create_Stored_Procedure_Command Nvarchar(Max);
Set @Create_Stored_Procedure_Command = 
N'Create Procedure dbo.Obsidian_Duplicate_Character_Cleaner_Parallel_Super_Fast
@Data Nvarchar(Max),
@Duplicate_Character Nvarchar(1),
@Maximum_Target_Processor_Parallel_Core_Count BigInt,
@Cleaned_Data Nvarchar(Max) Output
As
Set NoCount On;


--(USAGE GUIDANCE)
--Please note, that this stored procedure requires the execution of the serial version of this duplicate character cleaning technique to be already created in your database. Please find this function alongside in the directory where you obtained this query script on my Scarlett-Pimpernell Obsidian GitHub account
--The correct setting for the @Maximum_Target_Processor_Parallel_Core_Count parameter will be determined by the number of virtual or physical cores of the computer which you have deployed your SQL Server engine to. Please try and match them up if you want the most efficient and speedy execution. Otherwise, if you wamt to throttle the stored procedure execution in order to not overload the overall SQL Server engine server, you can set a significantly lower value for this variable. If in the odd case, there is some foolish person attempting to make use of this stored procedure, without fully understanding SQL Server, their best bet, is to just set this parameter value to around 20 to 30 cores, because there are not many typical servers that natively provide many multiples more physical/virtual cores than this... at the moment.
--For this particular implementation, I have also decided to simply artificially cap the core count to no more than half the @Data string length, because if you set the stored procedure parameters such that the max parallel core count is too high in comparison to the @Data string length, this can create far too many slices for parallel compute, and therefore results in computational inefficiency. Also please bare in mind, that if this max parallel core count is allowed to go above the acual physical or virtual number of cores availabe to the SQL Server engine, then it is likely that the processor will have to keep shuffling smaller than necessary slices in and out of the threads, which I am assuming is performance inhibiting to say the least.
--One shortcoming... I have not gotten round to the following, as I do not see it as important for 99.99999% of use cases... But, you could change the @Element_Index_Size variable calculation to only add 1, with a case statement, when the max parallel core count does not fit exactly into the nearest full divisor (which would be when not all slices would be full of the max parallel core count number of characters) of @Data length (minus 2 for the previously added extra zero paddings).

--(NOTA BENE)
--If you change the table variables to temporary tables, and add indexes here and there, this stored procedure may be even speedier in execution.
--If you use this stored procedure for relatively small @Data strings e.g. <500K characters long (maybe), instead of the serial version, then you may not experience any parallel compute speed gains. This algorithm is really intended for use with big @Data straings with lengths in excess of 500K characters (maybe), and then you will experience significant speed improvements in comparison to serial compute, so long as you have access to multiple processor cores, and set the max parallel core count parameter accordingly.
--This query contains a foundational parallel programming technique (slicing the input string depending on the ratio of the number of designated processor cores to the length of the input @Data string, and then applying a function on each slice simultaneously, followed by a string concatenation aggregation at the end, which returns the full transformed @Data string).
--This algorithm is ideal for big data compute use cases. Essentially, you could tweak the appropriate steps i.e. the application of the critical core string function in the @Data_Pattern_Delimit_Intermediate_B table variable insertion codeblock below (as well as the final double boundary methods), in order to enjoy significant parallel compute speed gains for a whole host of different string transformations!


--Data Validation Checks For Out Of Scope Parameters.
--(@Data)
If @Data Is Null GoTo Invalid_Data_Parameter_Value
If Len(Cast(@Data As Nvarchar(Max))) = 0 GoTo Invalid_Data_Parameter_Value
If @Data = '' ''  GoTo Invalid_Data_Parameter_Value
If @Data = ''''  GoTo Invalid_Data_Parameter_Value

--(@Duplicate_Character)
If @Duplicate_Character Is Null GoTo Invalid_Duplicate_Character_Parameter_Value
If Len(Cast(@Duplicate_Character As Nvarchar(Max))) = 0 GoTo Invalid_Duplicate_Character_Parameter_Value
If @Duplicate_Character = '' ''  GoTo Invalid_Duplicate_Character_Parameter_Value
If @Duplicate_Character = ''''  GoTo Invalid_Duplicate_Character_Parameter_Value
If Patindex(''%'' + Cast(@Duplicate_Character As Nvarchar(1)) + Cast(@Duplicate_Character As Nvarchar(1)) + ''%'', Cast(@Data As Nvarchar(Max))) = 0 GoTo Invalid_Duplicate_Character_Parameter_Value

--(@Maximum_Target_Processor_Parallel_Core_Count)
If @Maximum_Target_Processor_Parallel_Core_Count Is Null GoTo Invalid_Maximum_Target_Processor_Parallel_Core_Count_Parameter_Value
If Len(Cast(@Maximum_Target_Processor_Parallel_Core_Count As Nvarchar(Max))) = 0 GoTo Invalid_Maximum_Target_Processor_Parallel_Core_Count_Parameter_Value
If @Maximum_Target_Processor_Parallel_Core_Count = '' ''  GoTo Invalid_Maximum_Target_Processor_Parallel_Core_Count_Parameter_Value
If @Maximum_Target_Processor_Parallel_Core_Count = ''''  GoTo Invalid_Maximum_Target_Processor_Parallel_Core_Count_Parameter_Value


--(Internal Variables Part Two)
--Here I add a trailing zero to the @Data string in order to properly handle future joins.
Set @Data = (''0'' + @Data + ''0'')

Declare @Data_String_Length BigInt
Set @Data_String_Length = Len(Cast(@Data As Nvarchar(Max)))

--Here I simply place a lower cap on the Maximum_Target_Processor_Parallel_Core_Count variable.
Set @Maximum_Target_Processor_Parallel_Core_Count = (Case When @Maximum_Target_Processor_Parallel_Core_Count < 1 Then 1 Else @Maximum_Target_Processor_Parallel_Core_Count End)

--Here we invert the core count variable, so that as this variable increases, the number of data slices later on also increases. I have also implemented the max parallel core count cap here too.
Declare @Inverted_Maximum_Target_Processor_Parallel_Core_Count BigInt
Set @Inverted_Maximum_Target_Processor_Parallel_Core_Count = (@Data_String_Length / (Case When @Maximum_Target_Processor_Parallel_Core_Count > (@Data_String_Length / 2) Then (@Data_String_Length / 2) Else @Maximum_Target_Processor_Parallel_Core_Count End))

Declare @Element_Index_Size BigInt
Set @Element_Index_Size = ((Case When (@Data_String_Length / @Inverted_Maximum_Target_Processor_Parallel_Core_Count) < 1 Then 1 Else (@Data_String_Length / @Inverted_Maximum_Target_Processor_Parallel_Core_Count) End) + 1)


--The following codeblocks produce dynamically structured code in order to transform the @Data variable into a delimited character table.
Declare @Dynamic_SQL_1 Nvarchar(Max)
Set @Dynamic_SQL_1 = N''(Select dbo.Obsidian_Element_Index_Generator_Native_Code(@Parameter_1))''

Declare @Dynamic_SQL_Parameter_Definition_1 Nvarchar(Max)
Set @Dynamic_SQL_Parameter_Definition_1 = N''@Parameter_1 BigInt''

Declare @Dynamic_SQL_Intermediate_Table_Variable_1 Table ([Intermediate_SQL] Nvarchar(Max))
Insert Into @Dynamic_SQL_Intermediate_Table_Variable_1 ([Intermediate_SQL])
Execute sp_executesql @Dynamic_SQL_1, @Dynamic_SQL_Parameter_Definition_1, @Parameter_1 = @Element_Index_Size

Declare @Dynamic_SQL_Data_Intermediate_Variable Nvarchar(Max)
Set @Dynamic_SQL_Data_Intermediate_Variable = (Select Replace([Intermediate_SQL], ''@Index_Count'', @Element_Index_Size) From @Dynamic_SQL_Intermediate_Table_Variable_1)

Declare @Obsidian_Data_Element_Index Table ([Element_Index] BigInt)
Insert Into @Obsidian_Data_Element_Index ([Element_Index]) --You could just use an existing table of integers here, which would greatly speed up the query for big data applications where for example billions, trillions, or quadrillions of integers are needed on a daily basis.
Execute (@Dynamic_SQL_Data_Intermediate_Variable)

Declare @Data_Pattern_Delimit_Intermediate_A Table ([Order] BigInt, [Element_Index] BigInt, [Pattern] Nvarchar(Max))
Insert Into @Data_Pattern_Delimit_Intermediate_A ([Order], [Element_Index], [Pattern])
Select
	Row_Number() Over(Order By (Select Null) Asc) As [Order],
	[Element_Index] As [Element_Index],
	Substring(@Data, (Case When [Element_Index] = 1 Then 1 Else ((([Element_Index] * @Inverted_Maximum_Target_Processor_Parallel_Core_Count) + 1) - @Inverted_Maximum_Target_Processor_Parallel_Core_Count) End), @Inverted_Maximum_Target_Processor_Parallel_Core_Count) As [Pattern]
From @Obsidian_Data_Element_Index

Declare @Data_Pattern_Delimit_Intermediate_B Table ([Order] BigInt, [Element_Index] BigInt, [Cleaned_Pattern] Nvarchar(Max))
Insert Into @Data_Pattern_Delimit_Intermediate_B ([Order], [Element_Index], [Cleaned_Pattern])
Select
	[Order] As [Order],
	[Element_Index] As [Element_Index],
	dbo.Obsidian_Duplicate_Character_Cleaner_Serial([Pattern], @Duplicate_Character) As [Cleaned_Pattern]
From @Data_Pattern_Delimit_Intermediate_A
Where [Pattern] Like (''%'' + @Duplicate_Character + @Duplicate_Character + ''%'')

--Now we have to remove duplicate character consecutive rows... Good fun eh!
Declare @Data_Pattern_Delimit_Intermediate_C Table ([Order] BigInt, [Element_Index] BigInt, [Cleaned_Pattern] Nvarchar(Max))
Insert Into @Data_Pattern_Delimit_Intermediate_C ([Order], [Element_Index], [Cleaned_Pattern])
Select
	A.[Order] As [Order],
	A.[Element_Index] As [Element_Index],
	A.[Cleaned_Pattern] As [Cleaned_Pattern]
From @Data_Pattern_Delimit_Intermediate_B A
Left Join @Data_Pattern_Delimit_Intermediate_B B
	On (A.[Cleaned_Pattern] Like @Duplicate_Character And B.[Element_Index] = (A.[Element_Index] - 1) And B.[Cleaned_Pattern] Like @Duplicate_Character)
Where B.[Order] Is Null

----Without the first select statement in the codeblock below, there would be a bug caused by some slices not containing at least a double consecutive duplicate character, and thereby triggering a validation error.
Declare @Data_Pattern_Delimit_Final Table ([Order] BigInt, [Element_Index] BigInt, [Hybrid_Pattern] Nvarchar(Max))
Insert Into @Data_Pattern_Delimit_Final ([Order], [Element_Index], [Hybrid_Pattern])
Select
	A.[Order] As [Order],
	A.[Element_Index] As [Element_Index],
	A.[Pattern] As [Hybrid_Pattern]
From @Data_Pattern_Delimit_Intermediate_A A
Left Join @Data_Pattern_Delimit_Intermediate_B B
	On (B.[Order] = A.[Order])
Where B.[Order] Is Null
Union All
Select
	E.[Order] As [Order],
	E.[Element_Index] As [Pattern_Index],
	E.[Cleaned_Pattern] As [Hybrid_Pattern]
From @Data_Pattern_Delimit_Intermediate_C E


--The following codeblocks return useful data from the stored procedure regarding either errors or cleaned data strings.
--Finally, we concatenate the slices using a string aggrregation technique, and remove any remaining possible slice boundary double and triple consecutive duplicate characters, using a double replace method.
Set @Cleaned_Data = Cast(Replace(Replace((Select String_Agg([Hybrid_Pattern], '''') Within Group (Order By [Element_Index] Asc) From @Data_Pattern_Delimit_Final), (@Duplicate_Character + @Duplicate_Character), @Duplicate_Character), (@Duplicate_Character + @Duplicate_Character), @Duplicate_Character) As Nvarchar(Max))

--And remove the leading and trailing zero padding.
Set @Cleaned_Data = Left(@Cleaned_Data, (Len(@Cleaned_Data) - 1))
Set @Cleaned_Data = Right(@Cleaned_Data, (Len(@Cleaned_Data) - 1))
Return

Invalid_Data_Parameter_Value:
Set @Cleaned_Data = ''The first input parameter (@Data), is out of scope for one of the following reasons... It is either null, is of zero length, is a space, or is empty.''
Return

Invalid_Duplicate_Character_Parameter_Value:
Set @Cleaned_Data = ''The second input parameter (@Duplicate_Character), is out of scope for one of the following reasons... It is either null, is of zero length, is a space, is empty, or does not match any slice of the first input parameter (@Data) when at least duplicated i.e. there is not at least one instance of the designated duplicate character parameter within the data parameter.''
Return

Invalid_Maximum_Target_Processor_Parallel_Core_Count_Parameter_Value:
Set @Cleaned_Data = ''The third input parameter (@Maximum_Target_Processor_Parallel_Core_Count), is out of scope for one of the following reasons... It is either null, is of zero length, is a space, or is empty.''
Return


';

If Not Exists (Select * From Sysobjects Where Name='Obsidian_Duplicate_Character_Cleaner_Parallel_Super_Fast' and Xtype='P') --Make sure this function is not in use for another proprietary technique outside the Obsidian repository.
Execute sp_executesql @Create_Stored_Procedure_Command;